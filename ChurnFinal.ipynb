{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!-- snippet from http://chris-said.io/2016/02/13/how-to-make-polished-jupyter-presentations-with-optional-code-visibility/ -->\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Churn Data of a Telecom company #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by Alexander Abstreiter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Exploration\" data-toc-modified-id=\"Exploration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploration</a></span></li><li><span><a href=\"#Principal-component-analysis\" data-toc-modified-id=\"Principal-component-analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Principal component analysis</a></span></li><li><span><a href=\"#Comparison-of-differently-preprocessed-datasets-for-classification\" data-toc-modified-id=\"Comparison-of-differently-preprocessed-datasets-for-classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Comparison of differently preprocessed datasets for classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Without-additional-balancing-techniques\" data-toc-modified-id=\"Without-additional-balancing-techniques-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Without additional balancing techniques</a></span></li><li><span><a href=\"#Using-class-weights-in-the-loss-function\" data-toc-modified-id=\"Using-class-weights-in-the-loss-function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Using class weights in the loss function</a></span></li><li><span><a href=\"#Using-Synthetic-Minority-Over-sampling-Technique\" data-toc-modified-id=\"Using-Synthetic-Minority-Over-sampling-Technique-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Using Synthetic Minority Over-sampling Technique</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#K-Nearest-Neighbors\" data-toc-modified-id=\"K-Nearest-Neighbors-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>K-Nearest Neighbors</a></span></li><li><span><a href=\"#Support-Vector-Machine\" data-toc-modified-id=\"Support-Vector-Machine-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Support-Vector Machine</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, a dataset on churn data of a Telecom company is analysed. It can be found here: https://www.kaggle.com/becksddf/churn-in-telecoms-dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains data on the customers of a Telecom company.\n",
    "Each row represents a customer and the columns contain customer’s attributes which are described in the following:\n",
    "- state: the state the user lives in\n",
    "- account length: the number of days the user has this account\n",
    "- area code: the code of the area the user lives in\n",
    "- phone number: the phone number of the user\n",
    "- international plan: true if the user has the international plan, otherwise false\n",
    "- voice mail plan: true if the user has the voice mail plan, otherwise false\n",
    "- number vmail messages: the number of voice mail messages the user has sent\n",
    "- total day minutes: total number of minutes the user has been in calls during the day\n",
    "- total day calls: total number of calls the user has done during the day\n",
    "- total day charge: total amount of money the user was charged by the Telecom company for calls during the day\n",
    "- total eve minutes: total number of minutes the user has been in calls during the evening\n",
    "- total eve calls: total number of calls the user has done during the evening\n",
    "- total eve charge: total amount of money the user was charged by the Telecom company for calls during the evening\n",
    "- total night minutes: total number of minutes the user has been in calls during the night\n",
    "- total night calls: total number of calls the user has done during the night\n",
    "- total night charge: total amount of money the user was charged by the Telecom company for calls during the night\n",
    "- total intl minutes: total number of minutes the user has been in international calls\n",
    "- total intl calls: total number of international calls the user has done\n",
    "- total intl charge: total amount of money the user was charged by the Telecom company for international calls\n",
    "- customer service calls: number of customer service calls the user has done\n",
    "- churn: true if the user terminated the contract, otherwise false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer churn is the loss of clients or customers. Predicting churn can help the Telecom company, so it can effectively focus a customer retention marketing program (e.g. a special offer) to the subset of clients which are most likely to change their carrier. Therefore, the “churn” column is chosen as target and the following predictive analysis is a supervised classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analysis was conducted in Python 3.7.0 using Jupyter Notebook which is a web application that allows you to create an interactive environment that contains live code, visualizations and text.\n",
    "In addition, the following packages were used:\n",
    "- pandas: pandas provides high-performance data structures and operations for manipulating numerical tables and time series.\n",
    "- numpy: NumPy provides scientific computing capabilities such as a powerful N-dimensional array object, linear algebra, and random number capabilities.\n",
    "- sklearn: scikit-learn provides tools for data mining and data analysis.\n",
    "- imblearn: imbalanced-learn provides a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance.\n",
    "- plotly: Plot.ly is a graphing library which can produce interactive graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scientific computing libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data mining libaries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA#, FastICA\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#plot libaries\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True) # to show plots in notebook\n",
    "\n",
    "# online plotly\n",
    "from plotly.plotly import plot, iplot\n",
    "plotly.tools.set_credentials_file(username='XXXXXXXXXXXXXX', api_key='XXXXXXXXXXXXXX')\n",
    "\n",
    "# offline plotly\n",
    "#from plotly.offline import plot, iplot\n",
    "\n",
    "# do not show any warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 17 # specify seed for reproducable results\n",
    "pd.set_option('display.max_columns', None) # prevents abbreviation (with '...') of columns in prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "RANDOM_FOREST_PARAMS = {\n",
    "    'clf__max_depth': [25, 50, 75],\n",
    "    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19) \n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__n_estimators': [100, 300, 500, 1000]\n",
    "}\n",
    "\n",
    "DECISION_TREE_PARAMS = {\n",
    "    'clf__max_depth': [25, 50, 75],\n",
    "    'clf__max_features': [\"sqrt\"], # just sqrt is used because values of log2 and sqrt are very similar for our number of features (10-19)\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    'clf__min_samples_split': [6, 10, 14],\n",
    "}\n",
    "\n",
    "LOGISTIC_REGRESSION_PARAMS = {\n",
    "    'clf__solver': ['liblinear'],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__penalty': ['l2', 'l1']\n",
    "}\n",
    "\n",
    "KNN_PARAMS = {\n",
    "    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n",
    "    'clf__weights': ['uniform', 'distance'],\n",
    "    'clf__p': [1, 2, 10]\n",
    "}\n",
    "\n",
    "KNN_PARAMS_UNIFORM = {\n",
    "    'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],\n",
    "    'clf__weights': ['uniform'],\n",
    "    'clf__p': [1, 2, 10]\n",
    "}\n",
    "\n",
    "SVM_PARAMS = [\n",
    "{\n",
    "    'clf__kernel': ['linear'],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "}, \n",
    "{\n",
    "    'clf__kernel': ['rbf'],\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3333 rows and 21 columns.\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(\"/Users/Alex/Downloads/churn_kaggle.csv\")\n",
    "\n",
    "print(\"The dataset has %d rows and %d columns.\" % df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no null/missing values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# check for null values in the dataset\n",
    "print(\"There are \" + (\"some\" if df.isnull().values.any() else \"no\")  + \" null/missing values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look at some data points to get a feeling what the values of the various columns look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account length</th>\n",
       "      <th>area code</th>\n",
       "      <th>phone number</th>\n",
       "      <th>international plan</th>\n",
       "      <th>voice mail plan</th>\n",
       "      <th>number vmail messages</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total day calls</th>\n",
       "      <th>total day charge</th>\n",
       "      <th>total eve minutes</th>\n",
       "      <th>total eve calls</th>\n",
       "      <th>total eve charge</th>\n",
       "      <th>total night minutes</th>\n",
       "      <th>total night calls</th>\n",
       "      <th>total night charge</th>\n",
       "      <th>total intl minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>total intl charge</th>\n",
       "      <th>customer service calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KS</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>382-4657</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>371-7191</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NJ</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>358-1921</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  account length  area code phone number international plan  \\\n",
       "0    KS             128        415     382-4657                 no   \n",
       "1    OH             107        415     371-7191                 no   \n",
       "2    NJ             137        415     358-1921                 no   \n",
       "\n",
       "  voice mail plan  number vmail messages  total day minutes  total day calls  \\\n",
       "0             yes                     25              265.1              110   \n",
       "1             yes                     26              161.6              123   \n",
       "2              no                      0              243.4              114   \n",
       "\n",
       "   total day charge  total eve minutes  total eve calls  total eve charge  \\\n",
       "0             45.07              197.4               99             16.78   \n",
       "1             27.47              195.5              103             16.62   \n",
       "2             41.38              121.2              110             10.30   \n",
       "\n",
       "   total night minutes  total night calls  total night charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "\n",
       "   total intl minutes  total intl calls  total intl charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "\n",
       "   customer service calls  churn  \n",
       "0                       1  False  \n",
       "1                       1  False  \n",
       "2                       0  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns \"state\", \"international plan\", \"voice mail plan\" and \"churn\" have String values. The latter three seem to have just the values \"yes\" or \"no\" and are therefore converted to 1 and 0 respectively.\n",
    "\n",
    "The \"state\" column is converted using the LabelEncoder, which replaces each unique label with a unique integer.\n",
    "In this case, a label encode is used instead of dummy variables because of the many distinct values, which when converted into dummy variables would mess up the for example the PCA and the feature importance of the tree-based models.\n",
    "\n",
    "The \"phone number\" column is removed, because every customer has its own phone number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    pre_df = df.copy()\n",
    "    \n",
    "    # Replace the spaces in the column names with underscores\n",
    "    pre_df.columns = [s.replace(\" \", \"_\") for s in pre_df.columns]\n",
    "    \n",
    "    # convert string columns to integers\n",
    "    pre_df[\"international_plan\"] = pre_df[\"international_plan\"].apply(lambda x: 0 if x==\"no\" else 1)\n",
    "    pre_df[\"voice_mail_plan\"] = pre_df[\"voice_mail_plan\"].apply(lambda x: 0 if x==\"no\" else 1)\n",
    "    pre_df = pre_df.drop([\"phone_number\"], axis=1)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pre_df['state'])\n",
    "    pre_df['state'] = le.transform(pre_df['state'])\n",
    "    \n",
    "    return pre_df, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>international_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>total_eve_minutes</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>customer_service_calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>265.1</td>\n",
       "      <td>110</td>\n",
       "      <td>45.07</td>\n",
       "      <td>197.4</td>\n",
       "      <td>99</td>\n",
       "      <td>16.78</td>\n",
       "      <td>244.7</td>\n",
       "      <td>91</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>107</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>161.6</td>\n",
       "      <td>123</td>\n",
       "      <td>27.47</td>\n",
       "      <td>195.5</td>\n",
       "      <td>103</td>\n",
       "      <td>16.62</td>\n",
       "      <td>254.4</td>\n",
       "      <td>103</td>\n",
       "      <td>11.45</td>\n",
       "      <td>13.7</td>\n",
       "      <td>3</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>137</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.4</td>\n",
       "      <td>114</td>\n",
       "      <td>41.38</td>\n",
       "      <td>121.2</td>\n",
       "      <td>110</td>\n",
       "      <td>10.30</td>\n",
       "      <td>162.6</td>\n",
       "      <td>104</td>\n",
       "      <td>7.32</td>\n",
       "      <td>12.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state  account_length  area_code  international_plan  voice_mail_plan  \\\n",
       "0     16             128        415                   0                1   \n",
       "1     35             107        415                   0                1   \n",
       "2     31             137        415                   0                0   \n",
       "\n",
       "   number_vmail_messages  total_day_minutes  total_day_calls  \\\n",
       "0                     25              265.1              110   \n",
       "1                     26              161.6              123   \n",
       "2                      0              243.4              114   \n",
       "\n",
       "   total_day_charge  total_eve_minutes  total_eve_calls  total_eve_charge  \\\n",
       "0             45.07              197.4               99             16.78   \n",
       "1             27.47              195.5              103             16.62   \n",
       "2             41.38              121.2              110             10.30   \n",
       "\n",
       "   total_night_minutes  total_night_calls  total_night_charge  \\\n",
       "0                244.7                 91               11.01   \n",
       "1                254.4                103               11.45   \n",
       "2                162.6                104                7.32   \n",
       "\n",
       "   total_intl_minutes  total_intl_calls  total_intl_charge  \\\n",
       "0                10.0                 3               2.70   \n",
       "1                13.7                 3               3.70   \n",
       "2                12.2                 5               3.29   \n",
       "\n",
       "   customer_service_calls  churn  \n",
       "0                       1  False  \n",
       "1                       1  False  \n",
       "2                       0  False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df, _ = preprocess_data(df)\n",
    "pre_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical overview of the data**\n",
    "\n",
    "The following statistical measures can be seen for each column using the describe-function of DataFrame of the pandas library:\n",
    "- count: number of samples\n",
    "- mean: the mean of this attribute among all samples\n",
    "- std: the standard deviation of this attribute\n",
    "- min: the minimal value of this attribute\n",
    "- 25%: the lower percentile\n",
    "- 50%: the median\n",
    "- 75%: the upper percentile\n",
    "- max: the maximal value of this attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account_length</th>\n",
       "      <th>area_code</th>\n",
       "      <th>international_plan</th>\n",
       "      <th>voice_mail_plan</th>\n",
       "      <th>number_vmail_messages</th>\n",
       "      <th>total_day_minutes</th>\n",
       "      <th>total_day_calls</th>\n",
       "      <th>total_day_charge</th>\n",
       "      <th>total_eve_minutes</th>\n",
       "      <th>total_eve_calls</th>\n",
       "      <th>total_eve_charge</th>\n",
       "      <th>total_night_minutes</th>\n",
       "      <th>total_night_calls</th>\n",
       "      <th>total_night_charge</th>\n",
       "      <th>total_intl_minutes</th>\n",
       "      <th>total_intl_calls</th>\n",
       "      <th>total_intl_charge</th>\n",
       "      <th>customer_service_calls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "      <td>3333.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.059406</td>\n",
       "      <td>101.064806</td>\n",
       "      <td>437.182418</td>\n",
       "      <td>0.096910</td>\n",
       "      <td>0.276628</td>\n",
       "      <td>8.099010</td>\n",
       "      <td>179.775098</td>\n",
       "      <td>100.435644</td>\n",
       "      <td>30.562307</td>\n",
       "      <td>200.980348</td>\n",
       "      <td>100.114311</td>\n",
       "      <td>17.083540</td>\n",
       "      <td>200.872037</td>\n",
       "      <td>100.107711</td>\n",
       "      <td>9.039325</td>\n",
       "      <td>10.237294</td>\n",
       "      <td>4.479448</td>\n",
       "      <td>2.764581</td>\n",
       "      <td>1.562856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.824911</td>\n",
       "      <td>39.822106</td>\n",
       "      <td>42.371290</td>\n",
       "      <td>0.295879</td>\n",
       "      <td>0.447398</td>\n",
       "      <td>13.688365</td>\n",
       "      <td>54.467389</td>\n",
       "      <td>20.069084</td>\n",
       "      <td>9.259435</td>\n",
       "      <td>50.713844</td>\n",
       "      <td>19.922625</td>\n",
       "      <td>4.310668</td>\n",
       "      <td>50.573847</td>\n",
       "      <td>19.568609</td>\n",
       "      <td>2.275873</td>\n",
       "      <td>2.791840</td>\n",
       "      <td>2.461214</td>\n",
       "      <td>0.753773</td>\n",
       "      <td>1.315491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.700000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>24.430000</td>\n",
       "      <td>166.600000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>14.160000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179.400000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>201.400000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>201.200000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.050000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>216.400000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>36.790000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>10.590000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>510.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>350.800000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>59.640000</td>\n",
       "      <td>363.700000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>30.910000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>17.770000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             state  account_length    area_code  international_plan  \\\n",
       "count  3333.000000     3333.000000  3333.000000         3333.000000   \n",
       "mean     26.059406      101.064806   437.182418            0.096910   \n",
       "std      14.824911       39.822106    42.371290            0.295879   \n",
       "min       0.000000        1.000000   408.000000            0.000000   \n",
       "25%      14.000000       74.000000   408.000000            0.000000   \n",
       "50%      26.000000      101.000000   415.000000            0.000000   \n",
       "75%      39.000000      127.000000   510.000000            0.000000   \n",
       "max      50.000000      243.000000   510.000000            1.000000   \n",
       "\n",
       "       voice_mail_plan  number_vmail_messages  total_day_minutes  \\\n",
       "count      3333.000000            3333.000000        3333.000000   \n",
       "mean          0.276628               8.099010         179.775098   \n",
       "std           0.447398              13.688365          54.467389   \n",
       "min           0.000000               0.000000           0.000000   \n",
       "25%           0.000000               0.000000         143.700000   \n",
       "50%           0.000000               0.000000         179.400000   \n",
       "75%           1.000000              20.000000         216.400000   \n",
       "max           1.000000              51.000000         350.800000   \n",
       "\n",
       "       total_day_calls  total_day_charge  total_eve_minutes  total_eve_calls  \\\n",
       "count      3333.000000       3333.000000        3333.000000      3333.000000   \n",
       "mean        100.435644         30.562307         200.980348       100.114311   \n",
       "std          20.069084          9.259435          50.713844        19.922625   \n",
       "min           0.000000          0.000000           0.000000         0.000000   \n",
       "25%          87.000000         24.430000         166.600000        87.000000   \n",
       "50%         101.000000         30.500000         201.400000       100.000000   \n",
       "75%         114.000000         36.790000         235.300000       114.000000   \n",
       "max         165.000000         59.640000         363.700000       170.000000   \n",
       "\n",
       "       total_eve_charge  total_night_minutes  total_night_calls  \\\n",
       "count       3333.000000          3333.000000        3333.000000   \n",
       "mean          17.083540           200.872037         100.107711   \n",
       "std            4.310668            50.573847          19.568609   \n",
       "min            0.000000            23.200000          33.000000   \n",
       "25%           14.160000           167.000000          87.000000   \n",
       "50%           17.120000           201.200000         100.000000   \n",
       "75%           20.000000           235.300000         113.000000   \n",
       "max           30.910000           395.000000         175.000000   \n",
       "\n",
       "       total_night_charge  total_intl_minutes  total_intl_calls  \\\n",
       "count         3333.000000         3333.000000       3333.000000   \n",
       "mean             9.039325           10.237294          4.479448   \n",
       "std              2.275873            2.791840          2.461214   \n",
       "min              1.040000            0.000000          0.000000   \n",
       "25%              7.520000            8.500000          3.000000   \n",
       "50%              9.050000           10.300000          4.000000   \n",
       "75%             10.590000           12.100000          6.000000   \n",
       "max             17.770000           20.000000         20.000000   \n",
       "\n",
       "       total_intl_charge  customer_service_calls  \n",
       "count        3333.000000             3333.000000  \n",
       "mean            2.764581                1.562856  \n",
       "std             0.753773                1.315491  \n",
       "min             0.000000                0.000000  \n",
       "25%             2.300000                1.000000  \n",
       "50%             2.780000                1.000000  \n",
       "75%             3.270000                2.000000  \n",
       "max             5.400000                9.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are hard to interpret in this format, so we create some graphs which visualize them in a better way. First, we look at the distribution of the our target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plotly.colors.DEFAULT_PLOTLY_COLORS\n",
    "churn_dict = {0: \"no churn\", 1: \"churn\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/8.embed\" height=\"400px\" width=\"400px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[\"churn\"].value_counts()\n",
    "\n",
    "data = [go.Bar(x=[churn_dict[x] for x in y.index], y=y.values, marker = dict(color = colors[:len(y.index)]))]\n",
    "layout = go.Layout(\n",
    "    title='Churn distribution',\n",
    "    autosize=False,\n",
    "    width=400,\n",
    "    height=400,\n",
    "    yaxis=dict(\n",
    "        title='#samples',\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='basic-bar3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn percentage is 14.491%.\n"
     ]
    }
   ],
   "source": [
    "churn_perc = df[\"churn\"].sum() * 100 / df[\"churn\"].shape[0]\n",
    "print(\"Churn percentage is %.3f%%.\" % churn_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have clearly more samples for customers without churn than for customers with churn. So we have a class imbalance for the target variable which could lead to predictive models which are biased towards the majority (i.e. no churn). In order to deal with this issue we will investigate into the use of oversampling when building the models.\n",
    "\n",
    "Next, we look at the churn distribution per state, to see how much the state influences our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_churn_df = df.groupby([\"state\", \"churn\"]).size().unstack()\n",
    "trace1 = go.Bar(\n",
    "    x=state_churn_df.index,\n",
    "    y=state_churn_df[0],\n",
    "    marker = dict(color = colors[0]),\n",
    "    name='no churn'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=state_churn_df.index,\n",
    "    y=state_churn_df[1],\n",
    "    marker = dict(color = colors[1]),\n",
    "    name='churn'\n",
    ")\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Churn distribution per state',\n",
    "    autosize=True,\n",
    "    barmode='stack',\n",
    "    margin=go.layout.Margin(l=50, r=50),\n",
    "    xaxis=dict(\n",
    "        title='state',\n",
    "        tickangle=45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='#samples',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='stacked-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some states have less proportion of customer with churn like AK, HI, IA and some have a higher proportion such as WA, MD and TX. This shows that we should incorporate the state into our further analysis, because it could be help to predict if a customer is going to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following interactive graph shows the distribution of each feature for customer with churn and for the ones without churn. The slider can be used to switch between the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pre_df[pre_df[\"churn\"] == 1]\n",
    "no_churn = pre_df[pre_df[\"churn\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/12.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_churn_trace(col, visible=False):\n",
    "    return go.Histogram(\n",
    "        x=churn[col],\n",
    "        name='churn',\n",
    "        marker = dict(color = colors[1]),\n",
    "        visible=visible,\n",
    "    )\n",
    "\n",
    "def create_no_churn_trace(col, visible=False):\n",
    "    return go.Histogram(\n",
    "        x=no_churn[col],\n",
    "        name='no churn',\n",
    "        marker = dict(color = colors[0]),\n",
    "        visible = visible,\n",
    "    )\n",
    "\n",
    "features_not_for_hist = [\"state\", \"phone_number\", \"churn\"]\n",
    "features_for_hist = [x for x in pre_df.columns if x not in features_not_for_hist]\n",
    "active_idx = 0\n",
    "traces_churn = [(create_churn_trace(col) if i != active_idx else create_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n",
    "traces_no_churn = [(create_no_churn_trace(col) if i != active_idx else create_no_churn_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n",
    "data = traces_churn + traces_no_churn\n",
    "\n",
    "n_features = len(features_for_hist)\n",
    "steps = []\n",
    "for i in range(n_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * len(data)],\n",
    "        label = features_for_hist[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active = active_idx,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    yaxis=dict(\n",
    "        title='#samples',\n",
    "        automargin=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='histogram_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting histogram is of the feature \"international_plan\". While the proportion of churn for customers which have the international plan is much higher than the proportion of churn for customers without.\n",
    "\n",
    "The histograms for the \"total_day_minutes\" and \"total_day_charge\" are very similar and we can see that the customer with a higher value for these two features are more likely to churn. Interestingly, this does not apply to the number of day calls, which means that these customers seem to do longer calls. The minutes, charge and #calls for other times of the day (i.e. evening, night) do not show different distributions for customers with churn and without churn.\n",
    "\n",
    "Another interesting pattern is shown by the \"total_intl_calls\" feature. The data for the customers with churn are more left skewed than the data of the customers of the customer who did not churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we take a look at the box plots for each feature. A box plot visualizes the following statistics:\n",
    "- median\n",
    "- the first quartile (Q1) and the third quartile (Q3) building the interquartile range (IQR)\n",
    "- the lower fence (Q1 - 1.5 * IQR) and the upper fence (Q3 + 1.5 * IQR)\n",
    "- the maximum and the minimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/30.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_box_churn_trace(col, visible=False):\n",
    "    return go.Box(\n",
    "        y=churn[col],\n",
    "        name='churn',\n",
    "        marker = dict(color = colors[1]),\n",
    "        visible=visible,\n",
    "    )\n",
    "\n",
    "def create_box_no_churn_trace(col, visible=False):\n",
    "    return go.Box(\n",
    "        y=no_churn[col],\n",
    "        name='no churn',\n",
    "        marker = dict(color = colors[0]),\n",
    "        visible = visible,\n",
    "    )\n",
    "\n",
    "features_not_for_hist = [\"state\", \"phone_number\", \"churn\"]\n",
    "features_for_hist = [x for x in pre_df.columns if x not in features_not_for_hist]\n",
    "# remove features with too less distinct values (e.g. binary features), because boxplot does not make any sense for them\n",
    "features_for_box = [col for col in features_for_hist if len(churn[col].unique())>5]\n",
    "\n",
    "active_idx = 0\n",
    "box_traces_churn = [(create_box_churn_trace(col) if i != active_idx else create_box_churn_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\n",
    "box_traces_no_churn = [(create_box_no_churn_trace(col) if i != active_idx else create_box_no_churn_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\n",
    "data = box_traces_churn + box_traces_no_churn\n",
    "\n",
    "n_features = len(features_for_box)\n",
    "steps = []\n",
    "for i in range(n_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * len(data)],\n",
    "        label = features_for_box[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active = active_idx,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    yaxis=dict(\n",
    "        title='value',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='box_slider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the box plot for the number of voice mail messages (\"number_vmail_messages\"), we can see that we have some outliers for the customers with churn, but most of them have send zero voice mail messages. The customers which did not churn instead tend to do more voice mail messages.<br/>\n",
    "Similar to our findings in the histograms, we can see also in the box plot that the median of the total day minutes and the total day charge for churn clients is higher than the one of no-churn clients.<br/>\n",
    "Looking at the total international calls (\"total_intl_calls\"), the box plot shows that both churn and no-churn  customers are doing a similar amount of international calls, but the churn-customers tend to do longer calls as the median of churn customers for the total international minutes is higher than for the no-churn customers.<br/>\n",
    "Finally, the plot for the number of customer service calls shows that clients with churn have a higher median and a higher variance for the customer service calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to investigate the pair-wise correlations between two variables $X$ and $Y$, we use the Pearson correlation. Let $\\sigma_X, \\sigma_Y$ be the standard deviation of X,Y and $\\text{cov}(X, Y) = E[(X-E[X])(Y-E[Y])]$. Then we can define the Pearson correlation as the following:<br/>\n",
    "$\\rho_{X, Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y}\\,$.\n",
    "\n",
    "To visualize these correlations we use a heatmap plot, in which high correlations are coloured more to the red and lower ones more to the blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/10.embed\" height=\"700px\" width=\"850px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = pre_df.corr()\n",
    "trace = go.Heatmap(z=corr.values.tolist(), x=corr.columns, y=corr.columns)\n",
    "data=[trace]\n",
    "layout = go.Layout(\n",
    "    title='Heatmap of pairwise correlation of the columns',\n",
    "    autosize=False,\n",
    "    width=850,\n",
    "    height=700,\n",
    "    yaxis=go.layout.YAxis(automargin=True),\n",
    "    xaxis=dict(tickangle=40),\n",
    "    margin=go.layout.Margin(l=0, r=200, b=200, t=80)\n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='labelled-heatmap1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a high correlation between the voice mail plan and the number of voice mail messages. It makes sense that customers with the voice mail plan also send more voice mail messages.<br/>\n",
    "However, the international plan is just slightly correlated with the total international minutes and the international charge.<br/>\n",
    "As seen also in our previous analysis, the total day charge and the total day minutes a very highly correlated. Probably, this Telecom company charges per minute. The same behavior can be seen for the evening, the night and the international calls.<br/>\n",
    "The highest correlation with the churn variable have the international plan, the total_day_charge, the total_day_minutes and the number of customer service calls.\n",
    "\n",
    "In order to reduce the dimensionality of our dataset, we can identify and remove duplicate features according to their pairwise correlation with others. For this, we conduct a clustering of the features using agglomerative hierarchical clustering with average linkage. This method starts by creating one cluster for each feature and by computing the pairwise distance/dissimilarity/similarity between all the clusters, which in our case is the correlation. Then it select the two clusters with the highest average correlation to be merged. In the next iteration, the next pair of clusters is selected to be merged. This process is repeated until we end up with one cluster.\n",
    "\n",
    "To visualize the clustering process a dendrogram is shown in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/24.embed\" height=\"600px\" width=\"800px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster import hierarchy as hc\n",
    "X = np.random.rand(10, 10)\n",
    "names = pre_df.columns\n",
    "inverse_correlation = 1 - abs(pre_df.corr())\n",
    "fig = ff.create_dendrogram(inverse_correlation.values, orientation='left', labels=names, colorscale=colors, linkagefun=lambda x: hc.linkage(x, 'average'))\n",
    "fig['layout'].update(dict(\n",
    "    title=\"Dendrogram of clustering the features according to correlation\",\n",
    "    width=800, \n",
    "    height=600,\n",
    "    margin=go.layout.Margin(l=180, r=50),\n",
    "    xaxis=dict(\n",
    "        title='distance',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='features',\n",
    "        automargin=True,\n",
    "    ),\n",
    "))\n",
    "iplot(fig, filename='dendrogram_corr_clustering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four feature pairs which are each clustered together at a very low distance are:\n",
    "- total_night_minutes and total_night_charge\n",
    "- total_eve_minutes and total_eve_charge\n",
    "- total_intl_minutes and total_intl_charge\n",
    "- total_day_minutes and total_day_charge\n",
    "\n",
    "We save the all the charge features as duplicate features to be removed for our reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the duplicate features for later usage\n",
    "duplicate_features = [\"total_day_charge\", \"total_eve_charge\", \"total_night_charge\", \"total_intl_charge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the dataset for the following analysis we split it into the target column and the other predictors. In addition, we standardize all features to avoid e.g. higher impact of features with higher absolute values in classifiers which are based on a distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into feature vectors and the target variable\n",
    "df_y = pre_df[\"churn\"]\n",
    "df_X = pre_df.drop([\"churn\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset (note: for decision tree/random forest it would not be needed)\n",
    "df_X_normed = (df_X - df_X.mean()) / df_X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert the variables of a dataset into a new set of variables which are linearly uncorrelated and called principal components. The principal components are ranked according to the variance of data along them. This technique can be used to reduce the dimensionality of the dataset by considering just the most important principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the dimensionality of a dataset $X$ with $n$ variables to a new dataset with $k$ variables using PCA the following steps have to be followed:\n",
    "* standardize the data\n",
    "* calculate the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "* sort the Eigenvectors according to their Eigenvalues in decreasing order\n",
    "* build the $n×k$-dimensional projection matrix $W$ by putting the top $k$ Eigenvectors into the columns of $W$\n",
    "* transform the dataset $X$ by multiplying it with $W$: $X_{t} = XW$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply PCA to the dataset in order to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the principal components\n",
    "pca = PCA(random_state=SEED)\n",
    "df_X_pca = pca.fit_transform(df_X_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(pca.explained_variance_) # total explained variance of all principal components\n",
    "var_exp = [(i / tot) * 100 for i in sorted(pca.explained_variance_, reverse=True)] # individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # cumulative explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we plot the Scree Plot to determine how many components we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_cum_var_exp = go.Bar(\n",
    "    x=list(range(1, len(cum_var_exp) + 1)), \n",
    "    y=var_exp,\n",
    "    name=\"individual explained variance\",\n",
    ")\n",
    "trace_ind_var_exp = go.Scatter(\n",
    "    x=list(range(1, len(cum_var_exp) + 1)),\n",
    "    y=cum_var_exp,\n",
    "    mode='lines+markers',\n",
    "    name=\"cumulative explained variance\",\n",
    "    line=dict(\n",
    "        shape='hv',\n",
    "    ))\n",
    "data = [trace_cum_var_exp, trace_ind_var_exp]\n",
    "layout = go.Layout(\n",
    "    title='Individual and Cumulative Explained Variance',\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='percentage of explained variance',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"principal components\",\n",
    "        dtick=1,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph that the first five components explain the most individual variance, followed by the next nine components which explain less variance. The last five components explain near no variance. We choose to reduce our dataset by using the first ten components which explain about 80% of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "df_X_reduced = np.dot(df_X_normed.values, pca.components_[:n_components,:].T)\n",
    "df_X_reduced = pd.DataFrame(df_X_reduced, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of differently preprocessed datasets for classification ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we apply one classifier to the different version of the dataset. The Random Forest classifier is used because it is considered as a great baseline model for most applications. It is described in more detail in the next chapter. The following different versions of the dataset are investigated:\n",
    "- full dataset\n",
    "- dataset with variables reduced by the clustering according to the correlation\n",
    "- dataset reduced by considering the first ten principal components after applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we apply two different methods to deal with the unbalanced target variable. First, we try adjusting the weights for the penalization when the classifier makes a mistake in the training and then we try to oversample the data using the Synthetic Minority Over-sampling Technique.\n",
    "\n",
    "For the evaluation of the classifiers on the different datasets, a hold-out test set is used, which has 20% of all the data. To account for the class imbalance of our target variable, we use the f1-score as our main evaluation metric.\n",
    "We define:\n",
    "- TP = #samples for which the prediction is positive and the true label is positive\n",
    "- FP = #samples for which the prediction is positive but the true label is negative\n",
    "- TN = #samples for which the prediction is negative and the true label is negative\n",
    "- FN = #samples for which the prediction is negative but the true label is positive\n",
    "\n",
    "Then we define the following:\n",
    "\n",
    "$\\text{precision} = \\frac{TP}{TP + FP} \\;\\;\\; \\text{and} \\;\\;\\; \\text{recall} = \\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the f1-score is given by the following equation:\n",
    "\n",
    "$F_{1}=2\\,\\frac{\\text{precision}\\, \\times \\,\\text{recall}}{\\text{precision} \\, +\\, \\text{recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every classifier has a set of hyperparameters, which can be tuned by training the classifier with different values for these hyperparameters and selecting the classifier with the best score. In order to estimate the performance of a classifier in a more reliable way, k-fold cross validation (CV) is used. In k-fold CV, the training set is divided into a k subsets. Then we train k times our classifier on different unions of k-1 subsets and calculate its score on the subset which was not used for training. Then the final score is calculated by averaging the score of each iteration. In detail, let $C_1, C_2, ... C_k$ be the indices of the samples in each of the $K$ parts of the dataset and let $n_k$ be the number of observations in part $k$. Then the score from the cross validation is computed as follows:\n",
    "\n",
    "$\\text{Score}_{CV(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n}\\text{Score}_k$.\n",
    "\n",
    "In our hyper parameter tuning the Score is the f1-Score defined above.\n",
    "\n",
    "To do this analysis, we use the sklearn.model_selection.GridSearchCV object, to which we pass a classifier, a dictionary of hyperparameters with values and a $k$-fold object. For a good trade-off between runtime and accuracy of the score we choose $k=5$, so the classifiers are trained on 80% of the train data in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# prints the best grid search scores along with their parameters.\n",
    "def print_best_grid_search_scores_with_params(grid_search, n=5):\n",
    "    if not hasattr(grid_search, 'best_score_'):\n",
    "        raise KeyError('grid_search is not fitted.')\n",
    "    print(\"Best grid scores on validation set:\")\n",
    "    indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n",
    "    means = grid_search.cv_results_['mean_test_score'][indexes]\n",
    "    stds = grid_search.cv_results_['std_test_score'][indexes]\n",
    "    params = np.array(grid_search.cv_results_['params'])[indexes]\n",
    "    for mean, std, params in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gridsearch_with_cv(clf, params, X_train, y_train, cv, smote=None):\n",
    "\n",
    "    if smote is None:\n",
    "        pipeline = Pipeline([('clf', clf)])\n",
    "    else:\n",
    "        pipeline = Pipeline([('sm', sm), ('clf', clf)])\n",
    "        \n",
    "    gs = GridSearchCV(pipeline, params, cv=kf, n_jobs=-1, scoring='f1', return_train_score=True)\n",
    "    gs.fit(X_train, y_train)\n",
    "    return gs\n",
    "\n",
    "def score_on_test_set(clfs, datasets):\n",
    "    scores = []\n",
    "    for c, (X_test, y_test) in zip(clfs, datasets):\n",
    "        scores.append(c.score(X_test, y_test))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set in proportion 4:1 for all differntly preprocessed datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X_normed, df_y, test_size=0.2, random_state=SEED)\n",
    "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(df_X_reduced, df_y, test_size=0.2, random_state=SEED)\n",
    "cols_without_duplicate = [x for x in df_X_normed.columns if x not in duplicate_features]\n",
    "X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(df_X_normed[cols_without_duplicate], df_y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the full train dataset: (2666, 19)\n",
      "Shape of the train dataset with reduced features (2666, 15)\n",
      "Shape of the transformed train dataset using the first 10 Principal Components (2666, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the full train dataset:\", X_train.shape)\n",
    "print(\"Shape of the train dataset with reduced features\", X_train_red.shape)\n",
    "print(\"Shape of the transformed train dataset using the first 10 Principal Components\", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=SEED)\n",
    "kf = StratifiedKFold(n_splits=5, random_state=SEED)\n",
    "clf_rf = RandomForestClassifier(random_state=SEED)\n",
    "clf_balanced = RandomForestClassifier(random_state=SEED, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without additional balancing techniques ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the Random Forest on the full dataset, on the dataset with reduced features and on the dataset transformed using the first ten principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 342 ms, total: 15.2 s\n",
      "Wall time: 6min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_full = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\n",
    "gs_red = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\n",
    "gs_pca = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\n",
    "gss_raw = [gs_full, gs_red, gs_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_raw = score_on_test_set(gss_raw, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using class weights in the loss function ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn library offers for all parametric classifiers a parameter class_weight, which can be set to \"balanced\". Then, mistakes are weighted inversely proportional to the class frequencies. This means that mistakes for the minority class are penalized more than mistakes made for the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 198 ms, total: 14.6 s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_full_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=None)\n",
    "gs_red_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=None)\n",
    "gs_pca_balanced = do_gridsearch_with_cv(clf_balanced, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=None)\n",
    "gss_balanced_weights = [gs_full_balanced, gs_red_balanced, gs_pca_balanced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_balanced_weights = score_on_test_set(gss_balanced_weights, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Synthetic Minority Over-sampling Technique ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Synthetic Minority Over-sampling Technique (SMOTE) algorithm applies KNN approach where it selects one of the k nearest neighbors and computes the vector between the original point and the selected neighbor. The difference is multiplied by random number between (0, 1) and it is added back to original point to obtain the new synthetic point. Geometrically, the synthetic point is somewhere on the line between the original point and its neighbor.\n",
    "\n",
    "In the following, we use the SMOTE implementation SMOTE of the imblearn.over_sampling library. Furthermore, we create a Pipeline of applying Smote and then training the classifier, so that it is executed in every fold of x-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 160 ms, total: 14 s\n",
      "Wall time: 10min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_full_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)\n",
    "gs_red_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_red, y_train_red, kf, smote=sm)\n",
    "gs_pca_smote = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train_pca, y_train_pca, kf, smote=sm)\n",
    "gss_smote = [gs_full_smote, gs_red_smote, gs_pca_smote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_smote = score_on_test_set(gss_smote, [(X_test, y_test), (X_test_red, y_test_red), (X_test_pca, y_test_pca)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score  dataset and method\n",
      "0.821     full dataset using SMOTE\n",
      "0.810     data set with reduced features using SMOTE\n",
      "0.805     full dataset without any balancing\n",
      "0.787     full dataset using balanced class weights\n",
      "0.767     data set with reduced features without any balancing\n",
      "0.729     data set with reduced features using balanced class weights\n",
      "0.602     dataset with first 10 principal components using SMOTE\n",
      "0.500     dataset with first 10 principal components without any balancing\n",
      "0.496     dataset with first 10 principal components using balanced class weights\n"
     ]
    }
   ],
   "source": [
    "dataset_strings = [\"full dataset\", \"data set with reduced features\", \"dataset with first 10 principal components\"]\n",
    "method_strings = [\"without any balancing\", \"using balanced class weights\", \"using SMOTE\"]\n",
    "\n",
    "result_strings = dict()\n",
    "for ms, results in zip(method_strings, [test_results_raw, test_results_balanced_weights, test_results_smote]):\n",
    "    for ds, res in zip(dataset_strings, results):\n",
    "        string = \"%.3f\" % res + \"     \" + ds + \" \" + ms\n",
    "        result_strings[string] = res\n",
    "        2\n",
    "result_strings = sorted(result_strings.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(\"F1 score  dataset and method\")\n",
    "for k, _ in result_strings:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier trained on the PCA-transformed dataset performs the worst. The best results are obtained using the full dataset with SMOTE, closely followed by the reduced features dataset with SMOTE. So using SMOTE achieved better results than applying balanced class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we use the full dataset and apply SMOTE in the following classification chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "def get_color_with_opacity(color, opacity):\n",
    "    return \"rgba(\" + color[4:-1] + \", %.2f)\" % opacity\n",
    "\n",
    "# partially based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"f1\", random_state=SEED)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    trace1 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean - train_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = get_color_with_opacity(colors[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean + train_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = get_color_with_opacity(colors[0], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=train_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Train score\",\n",
    "        line = dict(\n",
    "            color = colors[0],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    trace4 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean - test_scores_std, \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = get_color_with_opacity(colors[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    trace5 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean + test_scores_std, \n",
    "        showlegend=False,\n",
    "        fill=\"tonexty\",\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        hoverlabel = dict(\n",
    "            namelength=20\n",
    "        ),\n",
    "        line = dict(\n",
    "            width = 0.1,\n",
    "            color = get_color_with_opacity(colors[1], 0.4),\n",
    "        ),\n",
    "    )\n",
    "    trace6 = go.Scatter(\n",
    "        x=train_sizes, \n",
    "        y=test_scores_mean, \n",
    "        showlegend=True,\n",
    "        name=\"Test score\",\n",
    "        line = dict(\n",
    "            color = colors[1],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [trace1, trace2, trace3, trace4, trace5, trace6]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        yaxis=dict(\n",
    "            title='F1 Score',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"#Training samples\",\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.8,\n",
    "            y=0,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_importance, title):\n",
    "    trace1 = go.Bar(\n",
    "        x=feature_importance[:, 0],\n",
    "        y=feature_importance[:, 1],\n",
    "        marker = dict(color = colors[0]),\n",
    "        name='feature importance'\n",
    "    )\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=True,\n",
    "        margin=go.layout.Margin(l=50, r=100, b=150),\n",
    "        xaxis=dict(\n",
    "            title='feature',\n",
    "            tickangle=30\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='feature importance',\n",
    "            automargin=True,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $x_i$ as the $n$-dimensional feature vector of a given sample and $\\beta_{0},\\,\\boldsymbol{\\beta} = (\\beta_{1}, ..., \\beta_{n})^T$ as the model parameters. Then the logistic regression model is defined as:\n",
    "\n",
    "$P(Y=1 \\vert x_i)= \\frac{\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )}{1+\\text{exp}(\\beta_{0} + x_i^T\\boldsymbol{\\beta} )} = \\frac{1}{1+\\text{exp}(-(\\beta_{0} + x_i^T\\boldsymbol{\\beta} ))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of a logistic regression include the following ones, which can be passed to the LogisticRegression of sklearn.linear_model:\n",
    "- penalty: the norm used for penalization (default='l2')\n",
    "- C: the inverse of the regularization strength (default=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 275 ms, sys: 12.1 ms, total: 287 ms\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_lr = LogisticRegression(random_state=SEED)\n",
    "gs_lr = do_gridsearch_with_cv(clf_lr, LOGISTIC_REGRESSION_PARAMS, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.493 (+/-0.051) for {'clf__C': 10, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "0.493 (+/-0.050) for {'clf__C': 10, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "0.492 (+/-0.048) for {'clf__C': 1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "0.492 (+/-0.047) for {'clf__C': 1, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "0.491 (+/-0.044) for {'clf__C': 0.1, 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_score = gs_lr.score(X_test, y_test)\n",
    "y_pred_lr = gs_lr.predict(X_test)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "cm_lr = cm_lr.astype('float') / cm_lr.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the normalized confusion matrix is shown. It demonstrates the proportion of samples which are true churn and predicted as churn/no churn and the proportion of samples which are true no churn and predicted as churn/no churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted no churn</th>\n",
       "      <th>predicted churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true no churn</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true churn</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted no churn  predicted churn\n",
       "true no churn               0.756            0.244\n",
       "true churn                  0.212            0.788"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm_lr.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix of the Logistic Regression shows that the probability to predict the correct class is with 0.76 and 0.79 similar for both classes. This means that the Logistic Regression has just a slight bias towards predicting a customer as churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/38.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_lr.best_estimator_, \"Learning Curve of Logistic Regression\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the learning curve of the Logistic Regression on the train set we can not see a clear trend, but on the train set the score increases with the number of training examples. Trained on all training samples the f1-score on the train set and on the test set is very similar, so the Logistic Regression is not overfitting the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors algorithm (KNN) is a non-parametric method, which considers the K closest training examples to the point of interest for predicting its class. This is done by a simple majority vote over the K closest points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of KNN include the following ones, which can be passed to the KNeighborsClassifier of sklearn.neighbors:\n",
    "- n_neighbors: corresponds to K, the number of nearest neighbors considered for the prediction (default=5) \n",
    "- weights: \n",
    "  - if uniform, then all neighbors have the same weight for the voting (default)\n",
    "  - if distance, then the votes of the neighbors are weighted by the inverse of the distance for the voting\n",
    "- p: the power parameter for the Minkowski metric (default=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 61.1 ms, total: 1.13 s\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_knn = KNeighborsClassifier()\n",
    "gs_knn = do_gridsearch_with_cv(clf_knn, KNN_PARAMS, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.604 (+/-0.079) for {'clf__n_neighbors': 55, 'clf__p': 1, 'clf__weights': 'distance'}\n",
      "0.603 (+/-0.080) for {'clf__n_neighbors': 45, 'clf__p': 1, 'clf__weights': 'uniform'}\n",
      "0.602 (+/-0.090) for {'clf__n_neighbors': 55, 'clf__p': 1, 'clf__weights': 'uniform'}\n",
      "0.600 (+/-0.079) for {'clf__n_neighbors': 45, 'clf__p': 1, 'clf__weights': 'distance'}\n",
      "0.599 (+/-0.088) for {'clf__n_neighbors': 65, 'clf__p': 1, 'clf__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn_score = gs_knn.score(X_test, y_test)\n",
    "y_pred_knn = gs_knn.predict(X_test)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "cm_knn = cm_knn.astype('float') / cm_knn.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the normalized confusion matrix is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted no churn</th>\n",
       "      <th>predicted churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true no churn</th>\n",
       "      <td>0.864</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true churn</th>\n",
       "      <td>0.176</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted no churn  predicted churn\n",
       "true no churn               0.864            0.136\n",
       "true churn                  0.176            0.824"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm_knn.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN classifier shows a small bias towards predicting no churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/40.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_knn.best_estimator_, \"Learning Curve of KNN\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score of 1 on the train set is a bit strange on the first sight. This behavior is caused by using the \"distance\" weights for the vote of the neighbors. This means that in the majority vote every point has a vote weight equal to the inverse of its distance to the point of interest. Because the point of interest is in the train set the distance to the nearest point is 0. So the weight of the vote of this point is infinite and the prediction is therefore equal to the class of the point itself. This applies to all training samples which results in a f1-score of 1.\n",
    "\n",
    "To show a more interesting learning curve on the train set, we train another KNN classifier using uniform weights instead of the distance weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn_uni = KNeighborsClassifier()\n",
    "gs_knn_uniform = do_gridsearch_with_cv(clf_knn_uni, KNN_PARAMS_UNIFORM, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.603 (+/-0.080) for {'clf__n_neighbors': 45, 'clf__p': 1, 'clf__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_knn_uniform, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/57.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_knn_uniform.best_estimator_, \"Learning Curve of KNN with uniform weights\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve shows that the f1-score on the test set constantly increases, which makes sense because the KNN can consider more training examples and therefore it can do a more accurate prediction on the test points. The train score has also an increasing trend but does not improve between 692 and 1652 training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support-Vector Machine ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear Support-Vector Machine (SVM) finds the optimal hyperplane between the points of two classes such that the distance of the nearest points to the decision boundary is maximized. This distance is called margin.\n",
    "\n",
    "If the data set is not linearly separable, we can map the samples ${\\bf x}$ into a feature space of higher dimensions:\n",
    "${\\bf x} \\longrightarrow \\phi({\\bf x})$ in which the classes can be linearly separated. This results in a non-linear decision boundary in the original dimensions.\n",
    "\n",
    "As the vectors ${\\bf x}_i$ appear only in inner products in both the decision\n",
    "function and the learning law, the mapping function $\\phi({\\bf x})$ does not \n",
    "need to be explicitly specified. Instead, we define a so-called kernel function:\n",
    "\n",
    "$ K({\\bf x}_1,{\\bf x}_2)=\\phi({\\bf x}_1)^T\\phi({\\bf x}_2)$.\n",
    "\n",
    "In the gridsearch we consider the following two kernels:\n",
    "- linear kernel: $ K({\\bf x}_1,{\\bf x}_2) = {\\bf x}_1 \\cdot {\\bf x}_2$\n",
    "- radial basis function: $ K({\\bf x}_1,{\\bf x}_2) = exp(-\\gamma({\\Vert {\\bf x}_1 - {\\bf x}_2 \\Vert}^2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of a SVM include the following ones, which can be passed to the SVC of sklearn.svm:\n",
    "- C: the inverse of the regularization strength (default=1.0)\n",
    "- kernel: the kernel used (default='rbf')\n",
    "- gamma: The higher the gamma value it tries to exactly fit the training data set (default='auto_deprecated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.17 s, sys: 149 ms, total: 3.32 s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_svm = svm.SVC(random_state=SEED, probability=True)\n",
    "gs_svm = do_gridsearch_with_cv(clf_svm, SVM_PARAMS, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.632 (+/-0.059) for {'clf__C': 10, 'clf__gamma': 0.01, 'clf__kernel': 'rbf'}\n",
      "0.631 (+/-0.082) for {'clf__C': 0.1, 'clf__gamma': 0.1, 'clf__kernel': 'rbf'}\n",
      "0.620 (+/-0.074) for {'clf__C': 1, 'clf__gamma': 0.01, 'clf__kernel': 'rbf'}\n",
      "0.617 (+/-0.091) for {'clf__C': 100, 'clf__gamma': 0.01, 'clf__kernel': 'rbf'}\n",
      "0.613 (+/-0.108) for {'clf__C': 1, 'clf__gamma': 0.1, 'clf__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svm_score = gs_svm.score(X_test, y_test)\n",
    "y_pred_svm = gs_svm.predict(X_test)\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "cm_svm = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the normalized confusion matrix is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted no churn</th>\n",
       "      <th>predicted churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true no churn</th>\n",
       "      <td>0.918</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true churn</th>\n",
       "      <td>0.212</td>\n",
       "      <td>0.788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted no churn  predicted churn\n",
       "true no churn               0.918            0.082\n",
       "true churn                  0.212            0.788"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm_svm.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows that the SVM classifier has a clear bias towards predicting no churn. This is not desirable in our case, because we do not want to miss out on churn-customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/42.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_svm.best_estimator_, \"Learning Curve of SVM\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train score of the SVM does not improve with the number of training samples. The test score, however, improves for the first 692 samples but stays the same with more samples. This could be due to the fact that the decision boundary of the SVM depends just on the support vectors and therefore changes just in the case that the additional training samples are support vectors. Furthermore, the train score is always by more than 0.1 higher than the test score which means that the SVM is overfitting on the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree for classification consists of several splits, which determine for a input sample, the predicted class, which is a leaf node in the tree. The construction of the decision trees is done with a greedy algorithm, because the theoretical minimum of function exists but it is NP-hard to determine it, because number of partitions has a factorial growth.\n",
    "Specifically, a greedy top-down approach is used which chooses a variable at each step that best splits the set of items. For measuring the \"best\" different metrics can be used, which generally measure the homogeneity of the target variable within the subsets. For this analysis we consider the following two metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity: Let $j$ be the number of classes and $p_i$ the fraction of items of class $i$ in a subset $p$, for $i \\in \\{1,2,..., j\\}$. Then the gini impurity is defined as follows: $\\;I_G(p) = 1- \\sum_{i=1}^j {p_i}^2$.\n",
    "\n",
    "\n",
    "Information gain: It measures the reduction in entropy when applying the split. The entropy is defined as $H(t) = - \\sum_{i=1}^j p_i\\, \\text{log}_2\\,p_i$. Then we define the information gain to split $n$ samples in parent node $p$ into $k$ partitions, where $n_i$ is the number of samples in partition $i$ as $IG = H(p) - \\sum_{i = 1}^k \\frac{n_i}{n} H(i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of a Decision Tree include the following ones, which can be passed to the DecisionTreeClassifier of sklearn.tree:\n",
    "- criterion: the criterion which decides the feature and the value at the split (default='gini')\n",
    "- max_depth: the maximum depth of each tree (default=None)\n",
    "- min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 ms, sys: 23.5 ms, total: 274 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_dt = DecisionTreeClassifier(random_state=SEED)\n",
    "gs_dt = do_gridsearch_with_cv(clf_dt, DECISION_TREE_PARAMS, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.612 (+/-0.072) for {'clf__criterion': 'gini', 'clf__max_depth': 25, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 6}\n",
      "0.612 (+/-0.072) for {'clf__criterion': 'gini', 'clf__max_depth': 50, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 6}\n",
      "0.612 (+/-0.072) for {'clf__criterion': 'gini', 'clf__max_depth': 75, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 6}\n",
      "0.587 (+/-0.033) for {'clf__criterion': 'entropy', 'clf__max_depth': 25, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 6}\n",
      "0.586 (+/-0.033) for {'clf__criterion': 'entropy', 'clf__max_depth': 75, 'clf__max_features': 'sqrt', 'clf__min_samples_split': 6}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_dt_score = gs_dt.score(X_test, y_test)\n",
    "y_pred_dt = gs_dt.predict(X_test)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "cm_dt = cm_dt.astype('float') / cm_dt.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the normalized confusion matrix is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted no churn</th>\n",
       "      <th>predicted churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true no churn</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true churn</th>\n",
       "      <td>0.271</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted no churn  predicted churn\n",
       "true no churn               0.912            0.088\n",
       "true churn                  0.271            0.729"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm_dt.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the SVM, also the Decision Tree has some bias towards predicting no churn, resulting in a poor performance in classifying the true churn clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree class of sklearn also computes an importance value for each feature. This is done by weighting the decrease of impurity at each split by the probability of reaching this node for each node which split involves the feature of interest. Then these weighted decreases of impurity are summed up for each feature and this gives the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/48.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = np.array(sorted(zip(X_train.columns, gs_dt.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\n",
    "plot_feature_importance(feature_importance, \"Feature importance in the decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most important features for the Decision Tree are the customer service calls and the total day minutes. These were also found to be correlated with the target variable in the data exploration. Interestingly, the area code is the third most important feature, but the area code has near zero correlation with the churn column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/44.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Decision Tree\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree is highly overfitted as the train score is about 0.3 higher than the test score. Both the train and especially the test score have an increasing trend with the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is an ensemble model that fits a number of decision tree classifiers on various sub-samples of the dataset which are created by the use of bootstrapping. In the inference stage it uses a majority vote over all trees to obtain the prediction. This improves the predictive accuracy and controls over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of a random forest include the following ones, which can be passed to the RandomForestClassifier of sklearn.ensemble:\n",
    "- n_estimators: the number of trees \n",
    "- criterion: the criterion which decides the feature and the value at the split (default='gini')\n",
    "- max_depth: the maximum depth of each tree (default=None)\n",
    "- min_samples_split: the minimum number of samples in a node to be considered for further splitting (default=2)\n",
    "- max_features: the number of features which are considered for a split (default='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.71 s, sys: 115 ms, total: 6.83 s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_rf = RandomForestClassifier(random_state=SEED)\n",
    "gs_rf = do_gridsearch_with_cv(clf_rf, RANDOM_FOREST_PARAMS, X_train, y_train, kf, smote=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid scores on validation set:\n",
      "0.789 (+/-0.037) for {'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      "0.789 (+/-0.037) for {'clf__criterion': 'entropy', 'clf__max_depth': 75, 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      "0.788 (+/-0.044) for {'clf__criterion': 'entropy', 'clf__max_depth': 25, 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      "0.787 (+/-0.032) for {'clf__criterion': 'entropy', 'clf__max_depth': 75, 'clf__max_features': 'sqrt', 'clf__n_estimators': 100}\n",
      "0.787 (+/-0.032) for {'clf__criterion': 'entropy', 'clf__max_depth': 50, 'clf__max_features': 'sqrt', 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print_best_grid_search_scores_with_params(gs_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf_score = gs_rf.score(X_test, y_test)\n",
    "y_pred_rf = gs_rf.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "cm_rf = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the normalized confusion matrix is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted no churn</th>\n",
       "      <th>predicted churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true no churn</th>\n",
       "      <td>0.976</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true churn</th>\n",
       "      <td>0.188</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted no churn  predicted churn\n",
       "true no churn               0.976            0.024\n",
       "true churn                  0.188            0.812"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm_rf.round(3), index=[\"true no churn\", \"true churn\"], columns=[\"predicted no churn\", \"predicted churn\"])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classifier shows also some bias towards predicting no churn, but not as much as the Decision Tree. Furthermore the results are very good by misclassifying just very few test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Random Forest the feature importances are obtained by computing it for all trees and taking the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/52.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_rf = np.array(sorted(zip(X_train.columns, gs_rf.best_estimator_.named_steps['clf'].feature_importances_), key=lambda x: x[1], reverse=True))\n",
    "plot_feature_importance(feature_importance_rf, \"Feature importance in the Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature for the Random Forest are the number of customer service calls, the total day minutes and the total day charge. The latter two basically contain the same information as they are highly correlated. Other important features are if the user has an international plan and the total international calls. The area code was not found as important as in the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/46.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_learning_curve(gs_dt.best_estimator_, \"Learning Curve of the Random Forest\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the test score of the Random Forest constantly increases with the number of samples. The train score is with over 0.9 always very high and shows that also the Random Forest overfits on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of the different classification model, we consider several evalutation metric in addition to the in chapter 4 defined precision, recall and f1-score. Further, let TP, FP, TN, FN be defined as in chapter 4.\n",
    "\n",
    "**Accuracy**<br/>\n",
    "The accuracy is the percentage of samples classified correctly: $\\text{accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}$.\n",
    "\n",
    "**Area Under the Receiver Operating Characteristic curve (AUC)**<br/>\n",
    "To introduce this concept, we define the following two metrics:\n",
    "- True positive rate (TPR): this is the same as the recall: $FPR = \\text{recall} = \\frac{TP}{FN + TP}$\n",
    "- False positive rate (FPR): this corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points: $FPR = \\frac{FP}{TN + FP}$\n",
    "\n",
    "To plot the Receiver Operating Characteristic (ROC) curve we choose a number of different classification thresholds and compute the TPR and the FPR. So the curve shows the trade-off between these two. To combine the TPR and the FPR into one evaluation metric the area under the ROC curve (AUC) is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the ROC curves of the classifiers trained in the previous chapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# code partially from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n",
    "    trace1 = go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1], \n",
    "        showlegend=False,\n",
    "        mode=\"lines\",\n",
    "        name=\"\",\n",
    "        line = dict(\n",
    "            color = colors[0],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    data = [trace1]\n",
    "    aucs = []\n",
    "    for clf, string, c in zip(classifiers, legend, colors[1:]):\n",
    "        y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n",
    "        y_score = clf.predict_proba(X_test)\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(2):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        aucs.append(roc_auc['micro'])\n",
    "\n",
    "        trace = go.Scatter(\n",
    "            x=fpr['micro'], \n",
    "            y=tpr['micro'], \n",
    "            showlegend=True,\n",
    "            mode=\"lines\",\n",
    "            name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n",
    "            hoverlabel = dict(\n",
    "                namelength=30\n",
    "            ),\n",
    "            line = dict(\n",
    "                color = c,\n",
    "            ),\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title,\n",
    "        autosize=False,\n",
    "        width=550,\n",
    "        height=550,\n",
    "        yaxis=dict(\n",
    "            title='True Positive Rate',\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"False Positive Rate\",\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0.4,\n",
    "            y=0.06,\n",
    "        ),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return aucs, iplot(fig, filename=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mbpro/54.embed\" height=\"550px\" width=\"550px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = [gs_lr, gs_knn, gs_svm, gs_dt, gs_rf]\n",
    "classifier_names = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\"]\n",
    "auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", X_test, y_test)\n",
    "roc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Random Forest classifier has with 0.98 the highest AUC value, followed by the SVM, the KNN and the Decision Tree with 0.96, 0.92 and 0.91, respectively. The Logistic Regression performs worse with an AUC of just 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the accuracy, precision, recall and f1-score on the test set for every classifier. The following table shows the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "recalls = []\n",
    "precision = []\n",
    "results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
    "for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    row = []\n",
    "    row.append(accuracy_score(y_test, y_pred))\n",
    "    row.append(precision_score(y_test, y_pred))\n",
    "    row.append(recall_score(y_test, y_pred))\n",
    "    row.append(f1_score(y_test, y_pred))\n",
    "    row.append(auc)\n",
    "    row = [\"%.3f\" % r for r in row]\n",
    "    results_table.loc[name] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.859</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.889</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.955</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy precision recall     f1    auc\n",
       "Logistic Regression    0.760     0.321  0.788  0.456  0.812\n",
       "KNN                    0.859     0.470  0.824  0.598  0.922\n",
       "SVM                    0.901     0.583  0.788  0.670  0.958\n",
       "Decision Tree          0.889     0.549  0.729  0.626  0.908\n",
       "Random Forest          0.955     0.831  0.812  0.821  0.978"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the Logistic Regression has with about 0.32 a very low precision value, which means that when it predicts a customer to churn, it is just in 32% of the cases correct.<br/>\n",
    "Moreover, in accuracy the Decision Tree performs with 0.89 better than the KNN, which achieves 0.86. But in the AUC measure the KNN outperforms with 0.92 the Decision Tree with 0.91.<br/>\n",
    "The KNN even has the highest recall value, but it achieves poor results for the precision, which also makes its f1-score the second worst of all classifiers.<br/>\n",
    "The SVM has with 0.58 the second highest precision score but is far behind the best precision score of 0.83 of the Random Forest. This also impacts the f1-score for which the SVM also achieves the second highest with an value of 0.67, but again being worse than the Random Forest, which achieves a f1-score of 0.82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal was to identify clients which are likely to churn, so we can do special-purpose marketing strategies to avoid the churn event. For this we evaluated differently preprocessed datasets and different classifiers. The analysis has shown that the PCA transformation was not found to be useful. Instead, we suggest to use the whole dataset and apply a oversampling technique in order to deal with the unbalanced target variable. <br/>\n",
    "In the classification chapter we have trained several different classifiers, including a Logistic Regression, a K-Nearest Neighbors Classifier, a Support-Vector Machine, a Decision Tree and a Random Forest. It was found that the best performance in accuracy, as well as f1-score and AUC is achieved by the Random Forest. One of the most important predictors for the Random Forest is the number of customer service calls. This might imply that the company should improve its customer service. Another important feature is the total day minutes and the total day charge, which basically hold the same information. So the company could try to either lower its charge per minute for clients, which have many day minutes or it could offer flat rates for calls.<br/>\n",
    "Concluding, we suggest the Telecom company to use the Random Forest model to identify potential churn customers and according to the customers life-time value present them special offers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#999; background:#fff;\">\n",
    "Created with Jupyter, delivered by Fastly, rendered by Rackspace.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
